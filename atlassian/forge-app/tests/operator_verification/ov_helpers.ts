/**
 * OPERATOR VERIFICATION: Helper Utilities
 * 
 * Provides utilities for all verification levels:
 * - Digest computation (determinism checking)
 * - Output discovery and validation
 * - Claim verification
 * - Doc/code consistency checks
 * - Report generation
 */

import * as fs from 'fs';
import * as path from 'path';
import * as crypto from 'crypto';

// ============================================================================
// DIGEST & DETERMINISM
// ============================================================================

export interface VerificationRun {
  runIndex: number;
  timestamp: string;
  checks: CheckResult[];
  digest: string;
  diggestFragment: string;
}

export interface CheckResult {
  checkId: string;
  level: number;
  status: 'PASS' | 'FAIL' | 'UNKNOWN';
  reason: string;
  metrics: Record<string, any>;
  evidenceRefs: Array<{ file: string; lines?: string }>;
  error?: string;
}

/**
 * Compute SHA256 digest of normalized check output
 * Excludes runIndex, timestamp, and other runtime-variable fields
 */
export function computeCheckDigest(check: CheckResult): string {
  const normalized = JSON.stringify({
    checkId: check.checkId,
    level: check.level,
    status: check.status,
    reason: check.reason,
    // Note: metrics are included but ONLY deterministic ones
    // runIndex and timestamps excluded to ensure cross-run determinism
  });
  return crypto.createHash('sha256').update(normalized).digest('hex');
}

/**
 * Compute run digest from all checks
 */
export function computeRunDigest(checks: CheckResult[]): string {
  const sortedChecks = checks.sort((a, b) => a.checkId.localeCompare(b.checkId));
  const digests = sortedChecks.map(c => computeCheckDigest(c)).join('|');
  return crypto.createHash('sha256').update(digests).digest('hex');
}

// ============================================================================
// OUTPUT DISCOVERY & VALIDATION
// ============================================================================

/**
 * Discover all output emitters in src/** (files with export functions)
 * Returns list of files that likely emit reports/outputs
 */
export function discoverOutputEmitters(srcDir: string): string[] {
  const emitters: string[] = [];
  const exportPatterns = ['export_', '_export', 'report_', '_report', 'emit'];
  
  function scanDir(dir: string) {
    try {
      const files = fs.readdirSync(dir);
      for (const file of files) {
        const fullPath = path.join(dir, file);
        const stat = fs.statSync(fullPath);
        if (stat.isDirectory() && !file.startsWith('.')) {
          scanDir(fullPath);
        } else if (file.endsWith('.ts') && !file.endsWith('.test.ts')) {
          if (exportPatterns.some(p => file.includes(p))) {
            emitters.push(fullPath);
          }
        }
      }
    } catch (e) {
      // Skip inaccessible dirs
    }
  }
  
  scanDir(srcDir);
  return emitters;
}

/**
 * Validate output envelope structure (Phase 5 contract)
 * Returns { isValid, missing }
 */
export function validateOutputEnvelope(
  output: any
): { isValid: boolean; missing: string[] } {
  const required = [
    'completenessStatus',
    'missingDataList',
    'confidenceLevel',
  ];
  
  const missing: string[] = [];
  for (const field of required) {
    if (!(field in output)) {
      missing.push(field);
    }
  }
  
  // If incomplete, must be DEGRADED or BLOCKED
  if (output.completenessStatus === 'INCOMPLETE' && output.completenessStatus !== 'OK') {
    if (!['DEGRADED', 'BLOCKED'].includes(output.completenessStatus)) {
      missing.push('invalid_status_for_incomplete');
    }
  }
  
  return {
    isValid: missing.length === 0,
    missing,
  };
}

// ============================================================================
// CLAIM & DOCS VERIFICATION
// ============================================================================

/**
 * Parse CLAIMS_PROOF_CATALOG.md and extract claim table
 */
export function parseClaimsProofCatalog(
  filePath: string
): Array<{ claimId: string; status: string; evidence: string[] }> {
  const content = fs.readFileSync(filePath, 'utf-8');
  const claims: Array<{ claimId: string; status: string; evidence: string[] }> = [];
  
  // Simple table parser - assumes markdown table format
  const lines = content.split('\n');
  let inTable = false;
  
  for (const line of lines) {
    if (line.includes('|') && !inTable) inTable = true;
    if (!inTable || line.startsWith('|---')) continue;
    
    const parts = line.split('|').map(p => p.trim()).filter(p => p);
    if (parts.length >= 3) {
      // Assume: ID | Status | Evidence...
      claims.push({
        claimId: parts[0],
        status: parts[1],
        evidence: parts.slice(2),
      });
    }
  }
  
  return claims;
}

/**
 * Verify required documentation files exist
 */
export function verifyRequiredDocs(docDir: string): {
  present: string[];
  missing: string[];
} {
  const required = [
    'PRIVACY.md',
    'SECURITY.md',
    'DATA_RETENTION.md',
    'DATA_INVENTORY.md',
    'ACCESS_CONTROL.md',
    'INCIDENT_RESPONSE.md',
    'SUPPORT.md',
    'COMPLIANCE.md',
    'ENTERPRISE_READINESS.md',
    'TERMS.md',
    'CHANGELOG_POLICY.md',
    'SUBPROCESSORS.md',
    'PLATFORM_DEPENDENCIES.md',
  ];
  
  const present: string[] = [];
  const missing: string[] = [];
  
  for (const doc of required) {
    const fullPath = path.join(docDir, doc);
    if (fs.existsSync(fullPath)) {
      present.push(doc);
    } else {
      missing.push(doc);
    }
  }
  
  return { present, missing };
}

/**
 * Scan source code for forbidden patterns (console logs, fetch calls, etc)
 */
export function scanSourceCodeForViolations(
  srcDir: string,
  patterns: { name: string; regex: RegExp; allowInTests: boolean }[]
): Array<{ pattern: string; file: string; line: number; match: string }> {
  const violations: Array<{ pattern: string; file: string; line: number; match: string }> = [];
  
  function scanDir(dir: string) {
    try {
      const files = fs.readdirSync(dir);
      for (const file of files) {
        const fullPath = path.join(dir, file);
        const stat = fs.statSync(fullPath);
        
        if (stat.isDirectory() && !file.startsWith('.') && file !== 'node_modules') {
          scanDir(fullPath);
        } else if (file.endsWith('.ts') && !file.endsWith('.test.ts')) {
          const content = fs.readFileSync(fullPath, 'utf-8');
          const lines = content.split('\n');
          
          lines.forEach((line, idx) => {
            for (const pattern of patterns) {
              if (pattern.regex.test(line)) {
                violations.push({
                  pattern: pattern.name,
                  file: fullPath,
                  line: idx + 1,
                  match: line.trim(),
                });
              }
            }
          });
        }
      }
    } catch (e) {
      // Skip
    }
  }
  
  scanDir(srcDir);
  return violations;
}

/**
 * Extract storage key prefixes from source code
 */
export function extractStorageKeyPrefixes(srcDir: string): string[] {
  const prefixes = new Set<string>();
  const storageKeyPattern = /storage\.set\(['"`]([^'"`]+)['"` ]/g;
  
  function scanDir(dir: string) {
    try {
      const files = fs.readdirSync(dir);
      for (const file of files) {
        const fullPath = path.join(dir, file);
        const stat = fs.statSync(fullPath);
        
        if (stat.isDirectory() && !file.startsWith('.') && file !== 'node_modules') {
          scanDir(fullPath);
        } else if (file.endsWith('.ts')) {
          const content = fs.readFileSync(fullPath, 'utf-8');
          let match;
          while ((match = storageKeyPattern.exec(content)) !== null) {
            const key = match[1];
            const prefix = key.split(':')[0];
            prefixes.add(prefix);
          }
        }
      }
    } catch (e) {
      // Skip
    }
  }
  
  scanDir(srcDir);
  return Array.from(prefixes).sort();
}

// ============================================================================
// REPORT GENERATION
// ============================================================================

/**
 * Write verification run results to JSONL
 */
export function writeVerificationResults(
  outFile: string,
  run: VerificationRun
): void {
  const lines: string[] = [];
  for (const check of run.checks) {
    const record = {
      runIndex: run.runIndex,
      level: check.level,
      checkId: check.checkId,
      status: check.status,
      reason: check.reason,
      metrics: check.metrics,
      digestFragment: computeCheckDigest(check),
      evidenceRefs: check.evidenceRefs,
      error: check.error,
    };
    lines.push(JSON.stringify(record));
  }
  
  fs.appendFileSync(outFile, lines.join('\n') + '\n');
}

/**
 * Write digests to file
 */
export function writeDigests(outFile: string, digests: string[]): void {
  fs.writeFileSync(
    outFile,
    digests.map((d, i) => `Run ${i + 1}: ${d}`).join('\n')
  );
}

/**
 * Detect divergence between runs
 */
export function detectDivergence(digests: string[]): {
  identical: boolean;
  firstDivergence?: number;
  summary: string;
} {
  const unique = new Set(digests);
  if (unique.size === 1) {
    return {
      identical: true,
      summary: `All ${digests.length} runs identical`,
    };
  }
  
  const firstDigest = digests[0];
  for (let i = 1; i < digests.length; i++) {
    if (digests[i] !== firstDigest) {
      return {
        identical: false,
        firstDivergence: i,
        summary: `Divergence detected at run ${i + 1}`,
      };
    }
  }
  
  return {
    identical: false,
    summary: `${unique.size} distinct digests detected across ${digests.length} runs`,
  };
}

/**
 * Scan for forbidden language (no inference without evidence)
 */
export function scanForForbiddenLanguage(
  text: string
): Array<{ term: string; location: number; context: string }> {
  const forbidden = [
    'compliant',
    'secure',
    'safe',
    'guaranteed',
    'certified',
    'all good',
    'proven',
    'complete',
  ];
  
  const findings: Array<{ term: string; location: number; context: string }> = [];
  const lowerText = text.toLowerCase();
  
  for (const term of forbidden) {
    let idx = 0;
    while ((idx = lowerText.indexOf(term, idx)) !== -1) {
      const start = Math.max(0, idx - 30);
      const end = Math.min(text.length, idx + term.length + 30);
      findings.push({
        term,
        location: idx,
        context: text.substring(start, end),
      });
      idx += term.length;
    }
  }
  
  return findings;
}

export default {
  computeCheckDigest,
  computeRunDigest,
  discoverOutputEmitters,
  validateOutputEnvelope,
  parseClaimsProofCatalog,
  verifyRequiredDocs,
  scanSourceCodeForViolations,
  extractStorageKeyPrefixes,
  writeVerificationResults,
  writeDigests,
  detectDivergence,
  scanForForbiddenLanguage,
};
